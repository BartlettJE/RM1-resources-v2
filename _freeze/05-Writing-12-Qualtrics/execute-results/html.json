{
  "hash": "547c4fdba0f90b5da5b3c271b4e38a19",
  "result": {
    "engine": "knitr",
    "markdown": "::: {.cell layout-align=\"center\"}\n\n:::\n\n\n# Creating Questionnaires and Behavioural Tasks {#questionnaires_tasks}\n\nAlthough you will not be designing your own studies in RM1, you need to understand how we designed and built the projects you are working with. In future, it will be important you know how to create your own questionnaires and tasks using common tools and software once it comes to your dissertation and as a future employability skill. \n\nThe key principle to keep in mind is this is a measurement problem. As psychologists, we want to investigate human behaviour and to do that we need the most valid, reliable, and pragmatic measure of human behaviour. Sometimes that will be a questionnaire or survey, other times it will be a task. It is important you know first how researchers create them and second how you can make them yourself. \n\nIn this chapter, we outline common tools and software you will have access to or will hear about, then explain some key principles in creating questionnaires, behavioural tasks, and considerations for collecting data in-person compared to online. \n\n## Common tools and software\n\nThere are a range of common tools you will come across as you read journal articles and talk to different lecturers. One of the most important considerations though is making sure any tools and software are GDPR compliant for data protection. When it comes to your dissertation, your supervisor will help with this. In this course, all the tools we recommend are GDPR compliant. As a starting point, the university maintains a [software and online tools](https://www.gla.ac.uk/myglasgow/it/softwareandonlinetools/){target=\"_blank\"} page which outline all the major approved services. Just note that not all of the tools we outline below are included on this site as the page is mainly about the services that the university purchases. \n\n**Microsoft Forms**\n\nThis Microsoft product is included in everyone's Office 365 suite. It is one of the university's preferred tools and we recommend using it for simple surveys. You can ask for free text responses or pre-set response options like select a category or rating scales. \n\n**Qualtrics**\n\nThis is possibly the most common survey platform you will come across within universities and across different workplaces. It is incredibly popular for market research and feedback, so you will come across it even if you do not want to work in academia. It has more advanced functions than Microsoft Forms and it is what we use for the projects in RM1. You can add features like randomisation and branching between questions which makes it great for psychology research. \n\n**Gorilla**\n\nGorilla is an online experiment platform where you can create both surveys and a range of behavioural tasks. You can create simple experiments using the builder interface or with some Javascript programming, you can make incredibly complex experiments. If you just want to make surveys, then you are better off using Forms or Qualtrics. However, if you need behavioural tasks or build experiments alongside survey questions, it is probably the most user friendly tool to create online studies. \n\n**PsychoPy**\n\nPsychoPy is a piece of software for developing behavioural tasks or experiments. It it based on Python, so you can either interact through the user interface or write Python code to build experiments. The software is local to a computer to collect data in-person or you can integrate it with their Pavlovia system to run experiments through a web browser. \n\n**OpenSesame**\n\nOpenSesame is very similar to PsychoPy where it is a Python-based experiment builder featuring a user interface or the possibility to write Python code. There is also an online platform called OSWeb to run experiments in a browser. The two tools are very similar so it is more down to personal preference or what your supervisor already uses. \n\n**Matlab**\n\nMatlab is a programming language for generating experiments, stimuli, and analysing data. The university provides access and many researchers have experience using Matlab over Python or PsychoPy. It is popular in brain imaging research due to extensive libraries supporting processing pipelines, but again it will be down to your supervisor's experience for whether they would support you using Matlab for a project. \n\n**Prolific**\n\nOne further tool you will hear about is Prolific - a participant recruitment service. In contrast to the previous tools, you do not create studies within Prolific, but integrate it with something like Gorilla or Qualtrics. You pay participants and select inclusion criteria from their participant pool and direct them to your study via a URL. Prolific has become one of the most popular recruitment services as it is GDPR compliant and recognised for higher participant quality. Many psychology studies use something called Amazon Mechanical Turk but it is not approved by many UK universities and the participant quality can be questionable given the far lower payment guidelines. \n\n## Questionnaires and scales\n\nOne of the most common ways of measuring human behaviour is through questionnaires or surveys. These rely on self-report for participants to select a response most consistent with how they are currently feeling or identify with. Survey studies normally consist of one or more psychometric scales, demographic questions to record participants' characteristics, and potentially free-text responses.\n\n### Scales\n\nThe idea behind scales is that you can measure an element of human behaviour by inferring a construct or latent variable through responses to multiple items. These are often in the form of Likert scales where participants indicate agreement or disagreement to an item. Often this will be on a 1 to 7 scale where 1 corresponds to strongly disagree and 7 corresponds to strongly agree. Across the items, you then take the sum or mean of each item to create an overall scale score. The response options and summary statistic for the scale score varies scale to scale, so it is important you identify the instructions and scoring information. \n\n::: {.callout-important}\n#### Valid and reliable measures\n\nPsychometric scales are carefully designed to measure an element of human behaviour in a valid and reliable way. It takes a lot of time and participants to validate a scale and we could easily dedicate a whole course to psychometrics. If you want an introduction to the stages involved in scale development, we recommend @boateng_best_2018. \n\nYou do not want to make up your own scales on the fly, so you are looking out for measures described and cited in previous studies. They will normally cite a validation article where authors describe the steps to create and validate the scale. For example, @alter_vssl_2024 validated a new scale on the value of software to statistical learning. They started with 10 items and identified the most effective 7 items. They demonstrated the internal consistency of items to measure the construct and evaluated how it compares to similar measures. \n:::\n\nFor example, one common scale for measuring depressive symptoms is the [Center for Epidemiological Studies-Depression](https://www.apa.org/pi/about/publications/caregivers/practice-settings/assessment/tools/depression-scale){target=\"_blank\"}(CES-D). You can then find the [items and scoring information](https://www.apa.org/depression-guideline/epidemiologic-studies-scale.pdf){target=\"_blank\"} via the APA. Participants are prompted with instructions \"Below is a list of the ways you might have felt or behaved. Please tell me how often you have felt this way during the past week\" and they respond to 20 items, such as \"My sleep was restless\". Participants then have four response options: \n\n1. Rarely or none of the time (less than 1 day ) \n\n2. Some or a little of the time (1-2 days) \n\n3. Occasionally or a moderate amount of time (3-4 days) \n\n4. Most or all of the time (5-7 days)\n\nFor this scale, you take the sum of the items where the options relate to a score of 0 for response 1 and a score of 3 for response 4. The lowest possible score is 0 and the highest possible score is 60, with a higher score indicating more severe depression. \n\nThis chapter explains the principles behind the instructions and items behind a scale. In the accompanying lab / weekly activity, you will learn the practical side of recreating a scale in Qualtrics. \n\n::: {.callout-warning}\n#### Finding scales and restrictions\n\nFor academic research, you normally find the items and instructions in validation articles. The authors put in the work to develop the scales and you cite the article to acknowledge the source and credit. However, prepare for detective work to find all of the information. Not all validation articles provide all the instructions, items, and/or scoring instructions. Sometimes you will need to search the name of the scale and find information from other researchers. \n\nRelatedly, one thing to be aware of is that you can freely reuse most academic scales and cite the authors for acknowledge. However, some scales (particularly diagnostic tools) have quite restrictive licences meaning the creators expect you to pay for using the scale. This is normally quite obvious when searching for the details of scales. We would never expect you to pay for using scales, so if you are ever unsure about the restrictions on using a scale, make sure you discuss it with your supervisor or course lead.  \n:::\n\n### Demographics\n\nA psychology study often includes a one or more demographic questions to contextualise your sample for reporting, or they might be included as key predictors in your study. It is normally a good idea to put demographics at the end of your study before the debrief to avoid influencing other responses and making sure people respond to the key scales or tasks first. \n\nThe main consideration is what demographic questions to include. Common options are age and gender, but it depends on the topic of your study for what demographic information would be useful to report. You do not need to collect every piece of information you can think of as it will waste your and your participant's time if you are not going to use it. See what similar studies report as this will help compare your sample characteristics to the studies that influenced you, and consider what demographics are relevant to your study and research question. \n\nThe other consideration is making sure the demographics are inclusively and respectfully worded. For concepts like gender and ethnicity, you will see large variance in the categories available to participants, particularly in older articles. Take gender, it is normally a good idea to make it optional for people to respond to the question \"What gender do you identify with?\" and providing inclusive options over binary man/woman, such as: \n\n- Woman\n\n- Man\n\n- Non-binary\n\n- Other (and allow people to type a free-text response)\n\n- Prefer not to say\n\n### Free-text responses\n\nInstead of pre-set responses participants can choose from, you can also ask people for a free-text response to a given question. This could be asking someone a simple question like how old they are in years which takes two characters or it might ask for a longer typed response in a qualitative or mixed methods survey. Like demographic questions, it is just a question of thinking about what you need to address your research question to make sure it is worth your and your participant's time. You will learn more about designing questions and studies for qualitative methods in RM2. \n\n::: {.callout-warning}\n#### Add validation criteria\n\nOne mistake you often learn about the hard way is recognising how participants can make mistakes in the questions you give them. For example, if you want people's age in years and you give them a free-text box, could they type that in words and cause you data processing headaches? In James' PhD, one of his free-text responses asked participants how many cigarettes they smoked per day but some people's internet browsers would autocorrect it to a date, messing up how it was read into R.\n\nTo solve this, many tools like Qualtrics let you set validation criteria for what information is allowed in a box. For example, it must be a valid email address, or it must be numeric with minimum and maximum allowed values. Try and take advantage of this validation criteria to avoid headaches further down the line. \n:::\n\n## Behavioural tasks and experiments\n\nIn contrast to self-report questionnaires, another way to measure behaviour is through behavioural tasks or experiments. You will come across tasks all the time in behavioural psychology research, but here we loosely define them as any task or experiment where you manipulate stimuli over multiple trials and record responses. We do not have time to teach you how to create them yourself in this course, but we explain how people design them and provide some examples to help you understand the method sections of articles. We then provide some recommended resources to learn more which might be useful to refer back to if you need it for your dissertation. \n\n### The Stroop task\n\nTo help explain the idea, the most famous example of a behavioural task is probably the Stroop task. In each trial, participants see a word relating to a colour (e.g., Blue or Red) in the middle of the screen. The researcher manipulates the font colour of the word to either be congruent or incongruent. For example, the word Blue written in blue in a congruent trial:\n\n<p style=\"color:blue;\">Blue.</p>\n\nAlternatively, the word Blue written in red in an incongruent trial: \n\n<p style=\"color:red;\">Blue.</p>\n\nIn each trial, the participant must indicate the font colour of the word as quickly as possible using the keyboard. Over tens or hundreds of trials, the participants completes all the variations of words and whether the font colour is congruent or incongruent. What you tend to find is that on average, participants respond quicker to the congruent words than the incongruent words. The explanation is that there is a Stroop interference effect as reading the word is a quicker process than processing the font colour, so incongruent trials take additional processing time to respond.\n\nThis is the basic idea behind a behavioural task. You want to measure some kind of human behaviour, so you use a behavioural task to manipulate stimuli and measure responses to tap into that behaviour. In contrast to questionnaires which rely on self-report and introspection, behavioural tasks are trying to measure behaviour people may not be consciously aware of, like the subtle processing difference between congruent and incongruent colours. \n\nThere are many somewhat standardised tasks out there that have been designed to measure different elements of human behaviour, so like questionnaires, it is not usually a good idea to simply make up your own as you want it to be valid and reliable. This means you must understand how researchers report the tasks they use to potentially recreate yourself. \n\n::: {.callout-tip}\n#### Try this\n\nIf you have not tried the Stroop task before, you can complete a demo in your web browser via [PsyToolKit](https://www.psytoolkit.org/experiment-library/stroop.html){target=\"_blank\"}. In the demo, you complete 40 trials and it will tell you your average response time to congruent and incongruent trials to show the Stroop interference effect. \n:::\n\n### Understanding the design of tasks\n\nNow we have walked through the Stroop task which many people have experienced, imagine you identified a task you want to recreate from a published paper but you have not completed one yourself before. Where do you find out how to create them? In all empirical psychology articles, there will be a method section outlining how the authors conducted their study. If this is written well enough (and in an ideal world, they should share their task script or design in their supplementary materials), it should allow you to recreate their study as close as possible. In many studies that use behavioural tasks, the authors provide diagrams of how their tasks are designed, such as @fig-Rass-task from an EEG study by @RASS20141417. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A figure copied from Rass et al. (2014) showing the design of a Eriksen Flanker task.](images/Rass_2012_task.jpg){#fig-Rass-task fig-align='center' fig-alt='Task diagram showing an Eriksen Flanker task. Starting with a fixation cross, a congruent or incongruent stimulus, then positive or negative feedback after a response.' width=100%}\n:::\n:::\n\n\nThis task is called the Eriksen Flanker task. It follows a similar principle to the Stroop task as it aims to measure the impact of interference on task performance. However, instead of looking at word colour, it uses distracting information. The aim of the task is to identify the middle letter in a five letter string (there are many variations of this, such as using arrows instead of letters). The four outer letters are distractors, and on some trials they are congruent, and on others they are incongruent. Studies usually find that response times are slower in the incongruent condition than the congruent condition. \n\nWe will go through the diagram above step by step to decode the design. In this experiment, a trial consists of a central fixation cross which stays on the screen for a random interval between 150-250 milliseconds (ms). This randomness is usually introduced to stop participants just mindlessly clicking buttons to predictable stimuli. A stimulus then appears on the screen for 80ms. This period is sometimes called the Stimulus Onset Asynchrony (SOA), or for how long the stimuli remain on the screen. There are two conditions for the stimuli: congruent (HHHHH or SSSSS) or incongruent (SSHSS or HHSHH). For this task, the participant must identify the middle letter by pressing either the letter ‘s’ or ‘h’ on the keyboard. After the stimulus has disappeared, there is a blank screen where the participant has up to 800ms to provide a response. After the response, a blank screen is presented for 300ms. The participant is then provided feedback to let them know whether they pressed the correct button or not. A ‘+’ is shown for a correct response and a ‘-’ is shown for an incorrect response. Finally, an inter-trial interval (ITI; although confusingly this is called an inter-stimulus interval despite indicating the end of a trial) is shown on the screen for 500ms to indicate the end of a trial. \n\nNow that we know how one trial is structured, we can see how many times this is repeated to form a block of trials. In the method section, there are more details about how many trials are included. For the participants to understand they are completing the task accurately, they complete 20 trials in a practice block. The authors then explain that participants completed four blocks each containing 100 trials for a total of 400 trials. Between each block, there is a rest period for the participant, but it does not say how long this period is. For demonstration purposes, we will say they receive a short 30 second break. Finally, we know that there are an equal number of congruent and incongruent stimuli in each block. As we have two types of congruent and incongruent stimuli, we can take a good guess that each one of these is presented 25 times in each block. The authors provide us with a diagram of each trial, but we can visualise the structure of the whole experiment like @fig-Rass-blocks. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The block design from Rass et al. (2014) showing the number of trials per block.](images/Rass_2012_blocks.PNG){#fig-Rass-blocks fig-align='center' fig-alt='A diagram showing the number of trials per block in the Eriksen Flanker task. There is one practice block, then four blocks of 100 trials each.' width=100%}\n:::\n:::\n\n\nThis is the amount of information you need from an article to enable you to recreate the task the authors used. This is a  good example with the only missing information being the duration of the breaks; a relatively minor detail. You should be prepared to come across substantially less helpful authors that do not provide sufficient details. This is usually the case when it comes to tasks that use images. These are not normally shared or even described. Hopefully, this will also demonstrate the importance of fully describing your experiment in a report or dissertation. Try and imagine you are the other researcher trying to recreate the task from your instructions.\n\n::: {.callout-tip}\n#### Try this\n\nIn the previous chapter, we invited you to read the method section (pages 2 and 3) of [*Registered Replication Report: Testing Disruptive Effects of Irrelevant Speech on Visual-Spatial Working Memory*](https://jeps.efpsa.org/articles/10.5334/jeps.450){target=\"_blank\"} by Kvetnaya (2018). At that point, the materials, design, and procedure might have been quite difficult to follow. Now we have explained the basic principles of breaking down behavioural tasks, try and revisit Kvetnaya's method section to see if you can understand how they designed and presented their task. \n:::\n\n## Online vs in-person data collection\n\nHelpful resources: \n\n- https://www.prolific.com/resources/how-to-improve-data-quality-in-online-studies\n\n- https://experimentology.io/012-collection.html \n\n## Further resources\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}